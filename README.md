# lmunit-tutorial

This repository contains the code for running LMUnit tests in CircleCI to evaluate the quality of AI generated responses.

## Setup

1. Clone the repository
2. Install the required packages: `pip install -r requirements.txt`
3. Run the tests: `pytest --junitxml=junit.xml`

## Understanding the LMUnit API

The [LMUnit API](https://docs.contextual.ai/api-reference/lmunit/lmunit) accepts three parameters:
* A `query` that represents the prompt that was sent to a model
* A `response` that represents the response that was generated by the model
* A `unit_test` is a *natural language* assertion of what the `response` should contain.

The LMUnit API returns a score that represents how closely the model response satisfies the natural language unit test.

The natural language expression of evaluations replaces more traditional NLP metrics or custom-defined evaluation metrics.

## Running the natural language tests

The [evalset.jsonl](./evalset.jsonl) file contains a list of queries and natural language unit tests that can be sent to the LMUnit API and scored. It also contains corresponding knowledge chunks that the language model in our example, [Contextual AI's Grounded Language Model](https://contextual.ai/blog/introducing-grounded-language-model/), references when generating responses; these are not necessary for all use cases though.

You can run the tests by running the following command: `python test_lmunit.py`

In your CI pipeline, you should sample from your AI agent or LLM to get the response, and then send the query, sampled response, and unit tests to the LMUnit API. You can set the pass/fail criteria based on the sensitivity of the unit test (i.e., more important unit tests require higher thresholds to pass) or based on a significant change in the score from the current production version.