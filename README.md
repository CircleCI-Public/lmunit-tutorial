# lmunit-tutorial

This repository contains the code for running LMUnit tests in CircleCI to evaluate the quality of AI generated responses.

## Setup

1. Clone the repository
2. Install the required packages: `pip -r requirements.txt`
3. Run the tests: `python -m unittest discover -s tests`

## Understanding the LMUnit API

The LMUnit API accepts three parameters:
* A `query` that represents the prompt that was sent to a model
* A `response` that represents the response that was generated by the model
* A `unit_test` is a *natural language* assertion of what the `response` should contain.

The LMUnit API returns a score that reprsents how closely the model response matches the natural language unit test.

The natural language expression of evaluations replaces more traditional NLP metrics or custom defined evaluation metrics.

## Running the natural language tests

The [evalset.jsonl](./evalset.jsonl) file contains a list of queries, responses, and natural language unit tests that can be sent to the LMUnit API and scored.

You can run the tests by running the following command: `python test_lmunit.py`

In a larger application, you might structure the tests differently. For example if you wanted to regression test a common queries or prompts, you might only have the set of queries and the unit tests to run.

In your CI pipeline, you would sample from the AI model to get the response, send the query, sampled response, and unit tests to the LMUnit API, and then pass or fail based on a threshold or a significant change in the score from the current production version.

We've kept the example here simpler to show the usage of the LMUnit API.